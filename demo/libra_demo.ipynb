{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple demo for Libra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook provides a demo that supports multi-instance inference for Libra in an image-based conversation scenario. Before starting a conversation, there are two preparations that need to be made *by following the instructions provided in the README*:\n",
    "\n",
    "1. Ensure that you have correctly installed the required environments.\n",
    "2. Download the pretrained weights and organize them as follows:\n",
    "   \n",
    "```\n",
    "CHECKPOINTS/\n",
    "├── libra-11b-chat/\n",
    "└── ...\n",
    "```\n",
    "\n",
    "Now, to run the demo on your device, you need to make ***two*** modifications. We will highlight the specific modifications you should make below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "PROJECT_PATH = os.path.dirname(os.path.abspath('.'))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from libra.models.libra import LibraForCausalLM, LibraTokenizer\n",
    "from libra.data.datasets import conversation as conversation_lib\n",
    "from libra.data import LibraEvalImageProcessor\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modification 1:** Specify the ``PRETRAINED_PATH`` of Libra models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_PATH = \"CHECKPOINTS/Libra/libra-11b-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isabs(PRETRAINED_PATH):\n",
    "    PRETRAINED_PATH = os.path.join(PROJECT_PATH, PRETRAINED_PATH)\n",
    "CONVERSATION = conversation_lib.conv_templates[\"v1\"]\n",
    "NUM_VISION_TOKENS = 578\n",
    "IMAGE_PLACEHOLDER = (\" <img_ph>\"*(NUM_VISION_TOKENS)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image(path):\n",
    "   return PIL.Image.open(path)\n",
    "\n",
    "def preprocess(img):\n",
    "   img = LibraEvalImageProcessor(pretrained_path=os.path.join(PRETRAINED_PATH, \"openai-clip-vit-large-patch14-336\"))(img)\n",
    "   img = torch.unsqueeze(img, 0)\n",
    "   return img\n",
    "\n",
    "def get_image_input(img_path):\n",
    "   image  = preprocess(open_image(img_path))\n",
    "   image = image.to(DEVICE)\n",
    "   return image\n",
    "\n",
    "def get_batch_image_input(img_paths):\n",
    "   if len(img_paths) == 0:\n",
    "      return None\n",
    "   else:\n",
    "      images = []\n",
    "      for img_path in img_paths:\n",
    "         image = get_image_input(img_path)\n",
    "         images.append(image)\n",
    "      images = torch.cat(images)\n",
    "      return images\n",
    "\n",
    "\n",
    "def stack_reconstructions(inputs, titles=None):\n",
    "   if inputs is None or len(inputs) == 0:\n",
    "      return\n",
    "   size = inputs[0].size\n",
    "   w, h = size[0], size[1]\n",
    "   num_imgs = len(inputs)\n",
    "   if titles is None:\n",
    "      titles = [\"img_{}\".format(k) for k in range(num_imgs)]\n",
    "   img = Image.new(\"RGB\", (num_imgs*w, h))\n",
    "   for i, x in enumerate(inputs):\n",
    "      assert x.size == size\n",
    "      img.paste(x, (i*w,0))\n",
    "   for i, title in enumerate(titles):\n",
    "      ImageDraw.Draw(img).text((i*w, 0), f'{title}', (255, 255, 255))\n",
    "   return img\n",
    "\n",
    "def custom_to_pil(x):\n",
    "  x = x.detach().cpu()\n",
    "  x = torch.clamp(x, -1., 1.)\n",
    "  x = (x + 1.)/2.\n",
    "  x = x.permute(1,2,0).numpy()\n",
    "  x = (255*x).astype(np.uint8)\n",
    "  x = Image.fromarray(x)\n",
    "  if not x.mode == \"RGB\":\n",
    "    x = x.convert(\"RGB\")\n",
    "  return x\n",
    "\n",
    "def process_prompt(prompts: list, raw_inputs: bool = False):\n",
    "    text, image = [], []\n",
    "    for prompt in prompts:\n",
    "        if prompt.get(\"image\", None) is not None:\n",
    "            image.append(prompt[\"image\"])\n",
    "            if prompt.get(\"text\", None) is not None:\n",
    "               if '<img_ph>' not in prompt[\"text\"]:\n",
    "                  prompt[\"text\"] = IMAGE_PLACEHOLDER + \"\\n\" + prompt[\"text\"]\n",
    "        if prompt.get(\"text\", None) is not None:\n",
    "            if raw_inputs:\n",
    "               text.append(prompt[\"text\"])\n",
    "            else:\n",
    "               conv = CONVERSATION.copy()\n",
    "               conv.append_message(conv.roles[0], prompt[\"text\"])\n",
    "               conv.append_message(conv.roles[1], None)\n",
    "               processed_text = conv.get_prompt()\n",
    "               text.append(processed_text)\n",
    "    image = get_batch_image_input(image)\n",
    "    contiguous_ignore_sign = [False for _ in range(len(image))]\n",
    "    processed = {\"language\": text, \"vision\": image, \"contiguous_ignore_sign\": contiguous_ignore_sign}\n",
    "\n",
    "    return processed\n",
    "\n",
    "def process_results(results):\n",
    "   text_results = [r['language'] for r in results]\n",
    "   image_results = [r['vision'] for r in results]\n",
    "   imgs = []\n",
    "   for img_result in image_results:\n",
    "      if img_result is None or len(img_result) == 0:\n",
    "         imgs.append(None)\n",
    "      else:\n",
    "         imgs.append([custom_to_pil(r) for r in img_result])\n",
    "\n",
    "   display_imgs = [stack_reconstructions(item) for item in imgs]\n",
    "   i2t_results = [text_results, display_imgs]\n",
    "   return i2t_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LibraForCausalLM.from_pretrained(PRETRAINED_PATH).to(torch.bfloat16).to(DEVICE).eval()\n",
    "tokenizer = LibraTokenizer(PRETRAINED_PATH).to(DEVICE)\n",
    "tokenizer.text_tokenizer.padding_side = 'left'\n",
    "tokenizer.text_tokenizer.add_eos_token = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modification 2:** Provide your testing prompts and images below.\n",
    "\n",
    "If you want to discard the system message, set ``raw_inputs`` to True, otherwise False. ``raw_inputs=True`` is usually used to evaluate the pretrianed models before instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    # Prompts for Libra-Chat\n",
    "    {\n",
    "        \"text\": \"Why is this image funny?\",\n",
    "        \"image\": \"/home/yfxu/libra/images/cartoon_duck.jpg\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Describe this image.\",\n",
    "        \"image\": \"/home/yfxu/libra/images/libra_logo.jpg\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"What is unusual in this image?\",\n",
    "        \"image\": \"/home/yfxu/libra/images/taxi.jpg\",       \n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Simply put, the theory of relativity states that \",\n",
    "    },\n",
    "    \n",
    "    # Prompts for Libra-Base\n",
    "    # {\n",
    "    #     \"text\": \"\",\n",
    "    #     \"image\": \"/home/yfxu/libra/images/cartoon_duck.jpg\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"text\": \"\",\n",
    "    #     \"image\": \"/home/yfxu/libra/images/taxi.jpg\",       \n",
    "    # },   \n",
    "]\n",
    "temperature = 0.0\n",
    "raw_inputs = False\n",
    "max_new_tokens = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(process_prompt(prompts, raw_inputs=raw_inputs), padding=True)\n",
    "generate_ids = model.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    attention_mask=inputs[\"attention_mask\"], \n",
    "    vision_indices=inputs[\"vision_indices\"], \n",
    "    contiguous_signal=inputs[\"coninous_signal\"].to(torch.bfloat16),\n",
    "    do_sample=True if temperature > 0 else False,\n",
    "    temperature=temperature,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "results = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "i2t_results = process_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the k-th results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "print(\"Output text:\")\n",
    "print(i2t_results[0][k])\n",
    "print(\"Encoded image:\")\n",
    "i2t_results[1][k]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
