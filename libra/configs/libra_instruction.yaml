model:
  arch: mind_train_wrapper
  pretrained: outputs/libra-base

  # # old version
  # pretrained: CHECKPOINTS/llama-2-7b-chat-hf-libra
  # pretrained_weight: outputs/libra-base
  
  custom_kwargs:
    vision_prediction_mode: "1d"
    use_bridge: True
    concat_signals: True
    vision_embd_pdrop: 0.0
    vision_resid_pdrop: 0.0

  model_kwargs:
    frozen_language: False
    llm_lr_scale: 1.0

  tokenizer_kwargs:
    add_eos_token: False
    padding_side: right
    raw_output: True
    model_max_length: 2048
    
datasets:
  instruction:
    custom_params:
      version: "v1"
      tokenizer_name: CHECKPOINTS/llama-2-7b-chat-hf-libra
      num_img_tokens: 578
      image_size: 336
      enable_t2i: False
      image_aspect_ratio: pad
      is_multimodal: True
      pad_to_square_i2t: True

    vis_processor:
      train:
        name: "libra_image"
        pretrained_path: CHECKPOINTS/openai-clip-vit-large-patch14-336 # prepare following README
    build_info:
      images:
        storage: 'DATASETS/instruction/data/' # you need to specify your own instruction data
      annotations:
        storage: 'DATASETS/instruction/llava_v1_5_mix665k.json' # you need to specify your own instruction data

run:
  ## lr_scheduler
  lr_scheduler_type: "cosine"
  # lr_scheduler_type: "constant_with_warmup"
  learning_rate: 2e-5
  warmup_ratio: 0.05
  # warmup_steps: 2000
  weight_decay: 0.01


  ## optimizer: dafault AdamW
  adam_beta1: 0.9
  adam_beta2: 0.99
  adam_epsilon: 1e-8

  ## dataloader
  num_train_epochs: 1
  # max_steps: 7000
  per_device_train_batch_size: 2 # total batch size: (per_device_train_batch_size: 2) * (gradient_accumulation_steps: 8) * (num_device_per_node: 8) * (num_node: 1) = 128
  gradient_accumulation_steps: 8
  dataloader_num_workers: 16
  dataloader_pin_memory: True

  ## checkpointing
  save_strategy: "epoch"
  save_total_limit: 1

  ## logging
  logging_steps: 100
  log_level: "info"
  log_on_each_node: False
  logging_strategy: "steps"

  # deepspeed
  deepspeed: lavis/projects/meta/deepspeed_configs/ZeRO-3.json

  ## others
  max_grad_norm: 1.0
  report_to: tensorboard
  bf16: True
  tf32: True
  gradient_checkpointing: True
  seed: 42
  output_dir: "outputs/libra-chat"
  remove_unused_columns: False
  disable_tqdm: False